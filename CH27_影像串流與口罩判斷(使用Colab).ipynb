{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"CH27_影像串流與口罩判斷(使用Colab).ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"38LO2exxeLDR"},"source":["# 匯入Tensorflow"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IakgPPP2eLDT","executionInfo":{"status":"ok","timestamp":1630142894762,"user_tz":-480,"elapsed":3052,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}},"outputId":"74a4c7b1-a992-4ef8-a68e-8cf243aefdc1"},"source":["import tensorflow\n","if tensorflow.__version__.startswith('1.'):\n","    import tensorflow as tf\n","    from tensorflow.python.platform import gfile\n","else:\n","    import tensorflow as v2\n","    import tensorflow.compat.v1 as tf\n","    tf.disable_v2_behavior()\n","    import tensorflow.compat.v1.gfile as gfile\n","print(\"Tensorflow version:{}\".format(tf.__version__))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","Tensorflow version:2.6.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FS8jr9IOeLDU"},"source":["# 匯入其他套件"]},{"cell_type":"code","metadata":{"id":"CFpGQaEFeLDV","executionInfo":{"status":"ok","timestamp":1630142897231,"user_tz":-480,"elapsed":722,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}}},"source":["import cv2,time,PIL,io,html,os\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ge2OLUvKeniY"},"source":["# COLAB執行串流所需套件"]},{"cell_type":"code","metadata":{"id":"hSqFL734evCX","executionInfo":{"status":"ok","timestamp":1630142901301,"user_tz":-480,"elapsed":272,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}}},"source":["from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5oh3YDkfKOx"},"source":["# 影像串流初始化(Colab)"]},{"cell_type":"code","metadata":{"id":"MLF-4rPHfJc8","executionInfo":{"status":"ok","timestamp":1630142904954,"user_tz":-480,"elapsed":270,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}}},"source":["#從Javascript 擷取出圖片\n","def js_to_image(js_reply):\n","  \n","  #對圖片進行解碼\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  #轉換成ndarray\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  #轉換成BGR影像格式\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","def bbox_to_bytes(bbox_array):\n","  #轉換成RGBA的格式\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  bbox_PIL.save(iobuf, format='png')\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjtdEkOggMqn","executionInfo":{"status":"ok","timestamp":1630142909381,"user_tz":-480,"elapsed":416,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}}},"source":["def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNqVDChSeLDW"},"source":["# 恢復PB檔案函數"]},{"cell_type":"code","metadata":{"id":"5vZ9YpeReLDW","executionInfo":{"status":"ok","timestamp":1630142917811,"user_tz":-480,"elapsed":249,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}}},"source":["def model_restore_from_pb(pb_path,node_dict,GPU_ratio=None):\n","  tf_node_dict = dict()\n","  with tf.Graph().as_default():\n","    config = tf.ConfigProto(log_device_placement=True,\n","                            allow_soft_placement=True,\n","                            )\n","    if GPU_ratio is None:\n","      config.gpu_options.allow_growth = True  \n","    else:\n","      config.gpu_options.per_process_gpu_memory_fraction = GPU_ratio \n","\n","    sess_pb = tf.Session(config=config)\n","    with gfile.FastGFile(pb_path, 'rb') as f:\n","      content = f.read()\n","      graph_def = tf.GraphDef()\n","      graph_def.ParseFromString(content)\n","      sess_pb.graph.as_default()\n","      \n","      tf.import_graph_def(graph_def, name='')# 匯入計算圖\n","\n","    sess_pb.run(tf.global_variables_initializer())\n","    for key,value in node_dict.items():\n","      try:\n","          node = sess_pb.graph.get_tensor_by_name(value)\n","          tf_node_dict[key] = node\n","      except:\n","          print(\"節點名稱:{}不存在\".format(key))\n","    return sess_pb,tf_node_dict"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBt6ZTRmeLDW"},"source":["# 人臉偵測相關函數"]},{"cell_type":"code","metadata":{"id":"fgW1xUHheLDX","executionInfo":{"status":"ok","timestamp":1630142920929,"user_tz":-480,"elapsed":424,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}}},"source":["def generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios, offset=0.5):\n","  '''\n","  generate anchors.\n","  :param feature_map_sizes: list of list, for example: [[40,40], [20,20]]\n","  :param anchor_sizes: list of list, for example: [[0.05, 0.075], [0.1, 0.15]]\n","  :param anchor_ratios: list of list, for example: [[1, 0.5], [1, 0.5]]\n","  :param offset: default to 0.5\n","  :return:\n","  '''\n","  anchor_bboxes = list()\n","  for idx, feature_size in enumerate(feature_map_sizes):\n","    cx = (np.linspace(0, feature_size[0] - 1, feature_size[0]) + 0.5) / feature_size[0]\n","    cy = (np.linspace(0, feature_size[1] - 1, feature_size[1]) + 0.5) / feature_size[1]\n","    cx_grid, cy_grid = np.meshgrid(cx, cy)\n","    cx_grid_expend = np.expand_dims(cx_grid, axis=-1)\n","    cy_grid_expend = np.expand_dims(cy_grid, axis=-1)\n","    center = np.concatenate((cx_grid_expend, cy_grid_expend), axis=-1)\n","\n","    num_anchors = len(anchor_sizes[idx]) +  len(anchor_ratios[idx]) - 1\n","    center_tiled = np.tile(center, (1, 1, 2* num_anchors))\n","    anchor_width_heights = []\n","\n","    # different scales with the first aspect ratio\n","    for scale in anchor_sizes[idx]:\n","        ratio = anchor_ratios[idx][0] # select the first ratio\n","        width = scale * np.sqrt(ratio)\n","        height = scale / np.sqrt(ratio)\n","        anchor_width_heights.extend([-width / 2.0, -height / 2.0, width / 2.0, height / 2.0])\n","\n","    # the first scale, with different aspect ratios (except the first one)\n","    for ratio in anchor_ratios[idx][1:]:\n","        s1 = anchor_sizes[idx][0] # select the first scale\n","        width = s1 * np.sqrt(ratio)\n","        height = s1 / np.sqrt(ratio)\n","        anchor_width_heights.extend([-width / 2.0, -height / 2.0, width / 2.0, height / 2.0])\n","\n","    bbox_coords = center_tiled + np.array(anchor_width_heights)\n","    bbox_coords_reshape = bbox_coords.reshape((-1, 4))\n","    anchor_bboxes.append(bbox_coords_reshape)\n","  anchor_bboxes = np.concatenate(anchor_bboxes, axis=0)\n","  return anchor_bboxes\n","\n","def decode_bbox(anchors, raw_outputs, variances=[0.1, 0.1, 0.2, 0.2]):\n","    '''\n","    Decode the actual bbox according to the anchors.\n","    the anchor value order is:[xmin,ymin, xmax, ymax]\n","    :param anchors: numpy array with shape [batch, num_anchors, 4]\n","    :param raw_outputs: numpy array with the same shape with anchors\n","    :param variances: list of float, default=[0.1, 0.1, 0.2, 0.2]\n","    :return:\n","    '''\n","    anchor_centers_x = (anchors[:, :, 0:1] + anchors[:, :, 2:3]) / 2\n","    anchor_centers_y = (anchors[:, :, 1:2] + anchors[:, :, 3:]) / 2\n","    anchors_w = anchors[:, :, 2:3] - anchors[:, :, 0:1]\n","    anchors_h = anchors[:, :, 3:] - anchors[:, :, 1:2]\n","    raw_outputs_rescale = raw_outputs * np.array(variances)\n","    predict_center_x = raw_outputs_rescale[:, :, 0:1] * anchors_w + anchor_centers_x\n","    predict_center_y = raw_outputs_rescale[:, :, 1:2] * anchors_h + anchor_centers_y\n","    predict_w = np.exp(raw_outputs_rescale[:, :, 2:3]) * anchors_w\n","    predict_h = np.exp(raw_outputs_rescale[:, :, 3:]) * anchors_h\n","    predict_xmin = predict_center_x - predict_w / 2\n","    predict_ymin = predict_center_y - predict_h / 2\n","    predict_xmax = predict_center_x + predict_w / 2\n","    predict_ymax = predict_center_y + predict_h / 2\n","    predict_bbox = np.concatenate([predict_xmin, predict_ymin, predict_xmax, predict_ymax], axis=-1)\n","    return predict_bbox\n","\n","def single_class_non_max_suppression(bboxes, confidences, conf_thresh=0.2, iou_thresh=0.5, keep_top_k=-1):\n","    '''\n","    do nms on single class.\n","    Hint: for the specific class, given the bbox and its confidence,\n","    1) sort the bbox according to the confidence from top to down, we call this a set\n","    2) select the bbox with the highest confidence, remove it from set, and do IOU calculate with the rest bbox\n","    3) remove the bbox whose IOU is higher than the iou_thresh from the set,\n","    4) loop step 2 and 3, util the set is empty.\n","    :param bboxes: numpy array of 2D, [num_bboxes, 4]\n","    :param confidences: numpy array of 1D. [num_bboxes]\n","    :param conf_thresh:\n","    :param iou_thresh:\n","    :param keep_top_k:\n","    :return:\n","    '''\n","    if len(bboxes) == 0: return []\n","\n","    conf_keep_idx = np.where(confidences > conf_thresh)[0]\n","\n","    bboxes = bboxes[conf_keep_idx]\n","    confidences = confidences[conf_keep_idx]\n","\n","    pick = []\n","    xmin = bboxes[:, 0]\n","    ymin = bboxes[:, 1]\n","    xmax = bboxes[:, 2]\n","    ymax = bboxes[:, 3]\n","\n","    area = (xmax - xmin + 1e-3) * (ymax - ymin + 1e-3)\n","    idxs = np.argsort(confidences)\n","\n","    while len(idxs) > 0:\n","      last = len(idxs) - 1\n","      i = idxs[last]\n","      pick.append(i)\n","\n","      # keep top k\n","      if keep_top_k != -1:\n","          if len(pick) >= keep_top_k:\n","              break\n","\n","      overlap_xmin = np.maximum(xmin[i], xmin[idxs[:last]])\n","      overlap_ymin = np.maximum(ymin[i], ymin[idxs[:last]])\n","      overlap_xmax = np.minimum(xmax[i], xmax[idxs[:last]])\n","      overlap_ymax = np.minimum(ymax[i], ymax[idxs[:last]])\n","      overlap_w = np.maximum(0, overlap_xmax - overlap_xmin)\n","      overlap_h = np.maximum(0, overlap_ymax - overlap_ymin)\n","      overlap_area = overlap_w * overlap_h\n","      overlap_ratio = overlap_area / (area[idxs[:last]] + area[i] - overlap_area)\n","\n","      need_to_be_deleted_idx = np.concatenate(([last], np.where(overlap_ratio > iou_thresh)[0]))\n","      idxs = np.delete(idxs, need_to_be_deleted_idx)\n","\n","    return conf_keep_idx[pick]"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJfeOMn9eLDY"},"source":["# 人臉偵測類別"]},{"cell_type":"code","metadata":{"id":"6BHyqI7HeLDY","executionInfo":{"status":"ok","timestamp":1630142930937,"user_tz":-480,"elapsed":252,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}}},"source":["class face_detection():\n","  def __init__(self,face_pb_path,margin = 40):\n","      \n","    node_dict = {'input':'data_1:0',\n","            'detection_bboxes':'loc_branch_concat_1/concat:0',\n","            'detection_scores':'cls_branch_concat_1/concat:0'}\n","    conf_thresh = 0.5\n","    iou_thresh = 0.6\n","    #----anchors config\n","    feature_map_sizes = [[33, 33], [17, 17], [9, 9], [5, 5], [3, 3]]\n","    anchor_sizes = [[0.04, 0.056], [0.08, 0.11], [0.16, 0.22], [0.32, 0.45], [0.64, 0.72]]\n","    anchor_ratios = [[1, 0.62, 0.42]] * 5\n","    \n","    #----generate anchors\n","    anchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n","    # for inference , the batch size is 1, the model output shape is [1, N, 4],\n","    # so we expand dim for anchors to [1, anchor_num, 4]\n","    anchors_exp = np.expand_dims(anchors, axis=0)\n","    sess,face_node_dict = model_restore_from_pb(face_pb_path, node_dict)\n","    tf_input = face_node_dict['input']\n","    shape = tf_input.shape\n","    model_shape = [None,shape[1].value,shape[2].value,shape[3].value]\n","#         print(\"model_shape = \", model_shape)\n","    detection_bboxes = node_dict['detection_bboxes']\n","    detection_scores = node_dict['detection_scores']\n","    \n","    self.margin = margin\n","    self.conf_thresh = conf_thresh\n","    self.iou_thresh = iou_thresh\n","    self.anchors_exp = anchors_exp\n","    self.model_shape = model_shape\n","    self.tf_input = tf_input\n","    self.sess = sess\n","    self.detection_bboxes = detection_bboxes\n","    self.detection_scores = detection_scores\n","  def infer(self,img):\n","    coors = list()\n","    height,width,_ = img.shape\n","    #----image processing\n","    img_resized = cv2.resize(img, (self.model_shape[2], self.model_shape[1]))\n","    img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n","    img_resized = img_resized.astype('float32')\n","    img_resized /= 255\n","    \n","    #----mask detection\n","    feed_dict={self.tf_input: np.expand_dims(img_resized, axis=0)}\n","    y_bboxes_output, y_cls_output = self.sess.run([self.detection_bboxes, self.detection_scores],\n","                            feed_dict=feed_dict)\n","    y_bboxes = decode_bbox(self.anchors_exp, y_bboxes_output)[0]\n","    y_cls = y_cls_output[0]\n","    # To speed up, do single class NMS, not multiple classes NMS.\n","    bbox_max_scores = np.max(y_cls, axis=1)\n","    bbox_max_score_classes = np.argmax(y_cls, axis=1)\n","\n","    # keep_idx is the alive bounding box after nms.\n","    keep_idxs = single_class_non_max_suppression(y_bboxes,\n","                        bbox_max_scores,\n","                        conf_thresh=self.conf_thresh,\n","                        iou_thresh=self.iou_thresh,\n","                        )\n","    #====draw bounding box\n","    for idx in keep_idxs:\n","        conf = float(bbox_max_scores[idx])\n","        class_id = bbox_max_score_classes[idx]\n","        bbox = y_bboxes[idx]\n","        # clip the coordinate, avoid the value exceed the image boundary.\n","        xmin = max(0, int(bbox[0] * width - self.margin // 2))\n","        ymin = max(0, int(bbox[1] * height - self.margin // 2))\n","        xmax = min(int(bbox[2] * width + self.margin // 2), width)\n","        ymax = min(int(bbox[3] * height + self.margin // 2), height)\n","        coors.append((xmin,ymin,xmax,ymax))\n","        \n","    return coors"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvsH-asoeLDZ"},"source":["# 影像串流與口罩判斷的函數"]},{"cell_type":"code","metadata":{"id":"XFE8F2uSeLDZ","executionInfo":{"status":"ok","timestamp":1630142936063,"user_tz":-480,"elapsed":261,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}}},"source":["def mask_or_not(face_pb_path,mask_pb_path,margin=40):\n","  #----var\n","  # face_pb_path = \"face_detection.pb\"\n","  frame_count = 0\n","  bbox = ''\n","  FPS = \"Initialing\"\n","  label_html = 'Capturing...'\n","  nodename_dict = {\n","            'input': 'input:0',\n","            'keep_prob': 'keep_prob:0',\n","            'prediction': 'prediction:0'\n","           }\n","  label2classname_dict = {0:'no_mask',1:\"with_mask\"}\n","                                       \n","  #----影像串流初始化\n","  # cap, height, width = video_init(video_source)\n","  # print(\"影像高度:\",height)\n","  # print(\"影像寬度:\",width)\n","\n","  #----人臉偵測器初始化\n","  find_face = face_detection(face_pb_path,margin=margin)\n","\n","  #----Mask PB檔案初始化\n","  sess_infer,tf_node_dict = model_restore_from_pb(mask_pb_path,nodename_dict,GPU_ratio=None)\n","  \n","  #----取出推論的節點\n","  pb_prediction = tf_node_dict['prediction']\n","  pb_input = tf_node_dict['input']\n","  pb_keep_prob = tf_node_dict['keep_prob']\n","\n","  #----COLAB影像串流初始化\n","  video_stream()\n","  \n","  while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","      break\n","\n","    #----Javascrip回應轉換成圖片格式\n","    img = js_to_image(js_reply[\"img\"])\n","    #----建立透明的方框初始值\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    #----人臉偵測\n","    coors = find_face.infer(img)\n","    \n","    if len(coors):\n","      for coor in coors:\n","        xmin,ymin,xmax,ymax = coor#臉部區域座標\n","        #----擷取臉部區域\n","        img_face = img[ymin:ymax,xmin:xmax,:].copy()\n","        #----調整大小至80 x 80\n","        img_face = cv2.resize(img_face,(80,80))\n","        #----將三維資料轉換成四維資料\n","        img_face = np.expand_dims(img_face,axis=0)\n","        #----將數值型態從uint8轉換成float32\n","        img_face = img_face.astype(np.float32)\n","        #----資料標準化(Normalization)\n","        img_face /= 255\n","\n","        #----口罩偵測\n","        predictions = sess_infer.run(pb_prediction,\n","                      feed_dict={pb_input:img_face, pb_keep_prob:1})\n","        #----根據label轉換成類別名稱\n","        arg_predictions = np.argmax(predictions,axis=1)\n","        classname = label2classname_dict[arg_predictions[0]]\n","        #----根據類別名稱決定方框的顏色\n","        if classname == 'with_mask':#有戴口罩，方框為綠色\n","            color = (0, 255, 0)  # (R,G,B)\n","        else:#沒戴口罩，方框為紅色\n","            color = (255, 0, 0)  # (R,G,B)\n","        #----畫上方框\n","        cv2.rectangle(bbox_array, (xmin, ymin), (xmax, ymax), color, 2)\n","        #----標註上類別名稱\n","        # cv2.putText(影像, 文字, 座標, 字型, 大小, 顏色, 線條寬度, 線條種類)\n","        cv2.putText(bbox_array, classname, (xmin + 2, ymin - 2),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color)\n","         \n","    #----FPS的計算\n","    if frame_count == 0:\n","        t_start = time.time()\n","    frame_count += 1\n","    if frame_count >= 4:\n","        t_stop = time.time()\n","        FPS = \"FPS={}\".format(round(4 / (t_stop - t_start)))\n","        frame_count = 0\n","\n","    # cv2.putText(影像, 文字, 座標, 字型, 大小, 顏色, 線條寬度, 線條種類)\n","    cv2.putText(bbox_array, FPS, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n","\n","    #----bbox的處理\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    bbox = bbox_bytes"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZpHSNeheLDa"},"source":["# 函數的使用"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylmegkxQpMZF","executionInfo":{"status":"ok","timestamp":1630142968052,"user_tz":-480,"elapsed":24464,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}},"outputId":"4d511180-d132-4cc7-8d9e-64eb760e484b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SPbVx1hQeLDa","colab":{"base_uri":"https://localhost:8080/","height":199},"executionInfo":{"status":"ok","timestamp":1630143528041,"user_tz":-480,"elapsed":552582,"user":{"displayName":"Liao Johnny","photoUrl":"","userId":"10482561226672514216"}},"outputId":"09153253-860a-4326-e827-357cd940ab6b"},"source":["mydrive_path = os.path.join('drive','MyDrive','Python','Code','Jupyter','Book','CH27')\n","face_pb_path = os.path.join(mydrive_path,\"face_detection.pb\")\n","mask_pb_path = os.path.join(mydrive_path,\"infer_acc_0.99.pb\")\n","margin = 40\n","mask_or_not(face_pb_path,mask_pb_path,margin=margin)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Device mapping:\n","/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n","\n","WARNING:tensorflow:From <ipython-input-6-4ad041e82795>:13: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.gfile.GFile.\n","Device mapping:\n","/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/javascript":["\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]}]}