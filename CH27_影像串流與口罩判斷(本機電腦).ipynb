{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38LO2exxeLDR"
   },
   "source": [
    "# 匯入Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2028,
     "status": "ok",
     "timestamp": 1628907897064,
     "user": {
      "displayName": "Liao Johnny",
      "photoUrl": "",
      "userId": "10482561226672514216"
     },
     "user_tz": -480
    },
    "id": "IakgPPP2eLDT",
    "outputId": "e888f3f3-3b8c-4faa-94c7-6decf63e72b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JohnnyKavnie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Tensorflow version:2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "if tensorflow.__version__.startswith('1.'):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.platform import gfile\n",
    "else:\n",
    "    import tensorflow as v2\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "    import tensorflow.compat.v1.gfile as gfile\n",
    "print(\"Tensorflow version:{}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS8jr9IOeLDU"
   },
   "source": [
    "# 匯入其他套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1628907967482,
     "user": {
      "displayName": "Liao Johnny",
      "photoUrl": "",
      "userId": "10482561226672514216"
     },
     "user_tz": -480
    },
    "id": "CFpGQaEFeLDV"
   },
   "outputs": [],
   "source": [
    "import cv2,time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohjyIGWyeLDV"
   },
   "source": [
    "# 影像串流初始化(本機電腦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1628908085932,
     "user": {
      "displayName": "Liao Johnny",
      "photoUrl": "",
      "userId": "10482561226672514216"
     },
     "user_tz": -480
    },
    "id": "pvxyzqDheLDV"
   },
   "outputs": [],
   "source": [
    "def video_init(source=0):\n",
    "    '''\n",
    "    source:影像的來源，\n",
    "        1.若是USB camera或筆電內建camera，填數字0\n",
    "        2.若是影片(.mp4 or .avi)，填影片的路徑\n",
    "    '''\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    #----獲取影像的高度與寬度\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)#預設值 480\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)#預設值 640\n",
    "\n",
    "    return cap,int(height),int(width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNqVDChSeLDW"
   },
   "source": [
    "# 恢復PB檔案函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1628908416857,
     "user": {
      "displayName": "Liao Johnny",
      "photoUrl": "",
      "userId": "10482561226672514216"
     },
     "user_tz": -480
    },
    "id": "5vZ9YpeReLDW"
   },
   "outputs": [],
   "source": [
    "def model_restore_from_pb(pb_path,node_dict,GPU_ratio=None):\n",
    "    tf_node_dict = dict()\n",
    "    with tf.Graph().as_default():\n",
    "        config = tf.ConfigProto(log_device_placement=True,\n",
    "                                allow_soft_placement=True,\n",
    "                                )\n",
    "        if GPU_ratio is None:\n",
    "            config.gpu_options.allow_growth = True  \n",
    "        else:\n",
    "            config.gpu_options.per_process_gpu_memory_fraction = GPU_ratio \n",
    "\n",
    "        sess_pb = tf.Session(config=config)\n",
    "        with gfile.FastGFile(pb_path, 'rb') as f:\n",
    "            content = f.read()\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(content)\n",
    "            sess_pb.graph.as_default()\n",
    "            \n",
    "            tf.import_graph_def(graph_def, name='')  # 匯入計算圖\n",
    "\n",
    "        sess_pb.run(tf.global_variables_initializer())\n",
    "        for key,value in node_dict.items():\n",
    "            try:\n",
    "                node = sess_pb.graph.get_tensor_by_name(value)\n",
    "                tf_node_dict[key] = node\n",
    "            except:\n",
    "                print(\"節點名稱:{}不存在\".format(key))\n",
    "        return sess_pb,tf_node_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBt6ZTRmeLDW"
   },
   "source": [
    "# 人臉偵測相關函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fgW1xUHheLDX"
   },
   "outputs": [],
   "source": [
    "def generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios, offset=0.5):\n",
    "    '''\n",
    "    generate anchors.\n",
    "    :param feature_map_sizes: list of list, for example: [[40,40], [20,20]]\n",
    "    :param anchor_sizes: list of list, for example: [[0.05, 0.075], [0.1, 0.15]]\n",
    "    :param anchor_ratios: list of list, for example: [[1, 0.5], [1, 0.5]]\n",
    "    :param offset: default to 0.5\n",
    "    :return:\n",
    "    '''\n",
    "    anchor_bboxes = []\n",
    "    for idx, feature_size in enumerate(feature_map_sizes):\n",
    "        cx = (np.linspace(0, feature_size[0] - 1, feature_size[0]) + 0.5) / feature_size[0]\n",
    "        cy = (np.linspace(0, feature_size[1] - 1, feature_size[1]) + 0.5) / feature_size[1]\n",
    "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
    "        cx_grid_expend = np.expand_dims(cx_grid, axis=-1)\n",
    "        cy_grid_expend = np.expand_dims(cy_grid, axis=-1)\n",
    "        center = np.concatenate((cx_grid_expend, cy_grid_expend), axis=-1)\n",
    "\n",
    "        num_anchors = len(anchor_sizes[idx]) +  len(anchor_ratios[idx]) - 1\n",
    "        center_tiled = np.tile(center, (1, 1, 2* num_anchors))\n",
    "        anchor_width_heights = []\n",
    "\n",
    "        # different scales with the first aspect ratio\n",
    "        for scale in anchor_sizes[idx]:\n",
    "            ratio = anchor_ratios[idx][0] # select the first ratio\n",
    "            width = scale * np.sqrt(ratio)\n",
    "            height = scale / np.sqrt(ratio)\n",
    "            anchor_width_heights.extend([-width / 2.0, -height / 2.0, width / 2.0, height / 2.0])\n",
    "\n",
    "        # the first scale, with different aspect ratios (except the first one)\n",
    "        for ratio in anchor_ratios[idx][1:]:\n",
    "            s1 = anchor_sizes[idx][0] # select the first scale\n",
    "            width = s1 * np.sqrt(ratio)\n",
    "            height = s1 / np.sqrt(ratio)\n",
    "            anchor_width_heights.extend([-width / 2.0, -height / 2.0, width / 2.0, height / 2.0])\n",
    "\n",
    "        bbox_coords = center_tiled + np.array(anchor_width_heights)\n",
    "        bbox_coords_reshape = bbox_coords.reshape((-1, 4))\n",
    "        anchor_bboxes.append(bbox_coords_reshape)\n",
    "    anchor_bboxes = np.concatenate(anchor_bboxes, axis=0)\n",
    "    return anchor_bboxes\n",
    "\n",
    "def decode_bbox(anchors, raw_outputs, variances=[0.1, 0.1, 0.2, 0.2]):\n",
    "    '''\n",
    "    Decode the actual bbox according to the anchors.\n",
    "    the anchor value order is:[xmin,ymin, xmax, ymax]\n",
    "    :param anchors: numpy array with shape [batch, num_anchors, 4]\n",
    "    :param raw_outputs: numpy array with the same shape with anchors\n",
    "    :param variances: list of float, default=[0.1, 0.1, 0.2, 0.2]\n",
    "    :return:\n",
    "    '''\n",
    "    anchor_centers_x = (anchors[:, :, 0:1] + anchors[:, :, 2:3]) / 2\n",
    "    anchor_centers_y = (anchors[:, :, 1:2] + anchors[:, :, 3:]) / 2\n",
    "    anchors_w = anchors[:, :, 2:3] - anchors[:, :, 0:1]\n",
    "    anchors_h = anchors[:, :, 3:] - anchors[:, :, 1:2]\n",
    "    raw_outputs_rescale = raw_outputs * np.array(variances)\n",
    "    predict_center_x = raw_outputs_rescale[:, :, 0:1] * anchors_w + anchor_centers_x\n",
    "    predict_center_y = raw_outputs_rescale[:, :, 1:2] * anchors_h + anchor_centers_y\n",
    "    predict_w = np.exp(raw_outputs_rescale[:, :, 2:3]) * anchors_w\n",
    "    predict_h = np.exp(raw_outputs_rescale[:, :, 3:]) * anchors_h\n",
    "    predict_xmin = predict_center_x - predict_w / 2\n",
    "    predict_ymin = predict_center_y - predict_h / 2\n",
    "    predict_xmax = predict_center_x + predict_w / 2\n",
    "    predict_ymax = predict_center_y + predict_h / 2\n",
    "    predict_bbox = np.concatenate([predict_xmin, predict_ymin, predict_xmax, predict_ymax], axis=-1)\n",
    "    return predict_bbox\n",
    "\n",
    "def single_class_non_max_suppression(bboxes, confidences, conf_thresh=0.2, iou_thresh=0.5, keep_top_k=-1):\n",
    "    '''\n",
    "    do nms on single class.\n",
    "    Hint: for the specific class, given the bbox and its confidence,\n",
    "    1) sort the bbox according to the confidence from top to down, we call this a set\n",
    "    2) select the bbox with the highest confidence, remove it from set, and do IOU calculate with the rest bbox\n",
    "    3) remove the bbox whose IOU is higher than the iou_thresh from the set,\n",
    "    4) loop step 2 and 3, util the set is empty.\n",
    "    :param bboxes: numpy array of 2D, [num_bboxes, 4]\n",
    "    :param confidences: numpy array of 1D. [num_bboxes]\n",
    "    :param conf_thresh:\n",
    "    :param iou_thresh:\n",
    "    :param keep_top_k:\n",
    "    :return:\n",
    "    '''\n",
    "    if len(bboxes) == 0: return []\n",
    "\n",
    "    conf_keep_idx = np.where(confidences > conf_thresh)[0]\n",
    "\n",
    "    bboxes = bboxes[conf_keep_idx]\n",
    "    confidences = confidences[conf_keep_idx]\n",
    "\n",
    "    pick = []\n",
    "    xmin = bboxes[:, 0]\n",
    "    ymin = bboxes[:, 1]\n",
    "    xmax = bboxes[:, 2]\n",
    "    ymax = bboxes[:, 3]\n",
    "\n",
    "    area = (xmax - xmin + 1e-3) * (ymax - ymin + 1e-3)\n",
    "    idxs = np.argsort(confidences)\n",
    "\n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # keep top k\n",
    "        if keep_top_k != -1:\n",
    "            if len(pick) >= keep_top_k:\n",
    "                break\n",
    "\n",
    "        overlap_xmin = np.maximum(xmin[i], xmin[idxs[:last]])\n",
    "        overlap_ymin = np.maximum(ymin[i], ymin[idxs[:last]])\n",
    "        overlap_xmax = np.minimum(xmax[i], xmax[idxs[:last]])\n",
    "        overlap_ymax = np.minimum(ymax[i], ymax[idxs[:last]])\n",
    "        overlap_w = np.maximum(0, overlap_xmax - overlap_xmin)\n",
    "        overlap_h = np.maximum(0, overlap_ymax - overlap_ymin)\n",
    "        overlap_area = overlap_w * overlap_h\n",
    "        overlap_ratio = overlap_area / (area[idxs[:last]] + area[i] - overlap_area)\n",
    "\n",
    "        need_to_be_deleted_idx = np.concatenate(([last], np.where(overlap_ratio > iou_thresh)[0]))\n",
    "        idxs = np.delete(idxs, need_to_be_deleted_idx)\n",
    "\n",
    "    return conf_keep_idx[pick]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJfeOMn9eLDY"
   },
   "source": [
    "# 人臉偵測類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6BHyqI7HeLDY"
   },
   "outputs": [],
   "source": [
    "class face_detection():\n",
    "    def __init__(self,face_pb_path,margin = 40,GPU_ratio=None):\n",
    "        \n",
    "        node_dict = {'input':'data_1:0',\n",
    "                     'detection_bboxes':'loc_branch_concat_1/concat:0',\n",
    "                     'detection_scores':'cls_branch_concat_1/concat:0'}\n",
    "        conf_thresh = 0.8\n",
    "        iou_thresh = 0.7\n",
    "        #----anchors config\n",
    "        feature_map_sizes = [[33, 33], [17, 17], [9, 9], [5, 5], [3, 3]]\n",
    "        anchor_sizes = [[0.04, 0.056], [0.08, 0.11], [0.16, 0.22], [0.32, 0.45], [0.64, 0.72]]\n",
    "        anchor_ratios = [[1, 0.62, 0.42]] * 5\n",
    "        \n",
    "        #----generate anchors\n",
    "        anchors = generate_anchors(feature_map_sizes, anchor_sizes, anchor_ratios)\n",
    "        # for inference , the batch size is 1, the model output shape is [1, N, 4],\n",
    "        # so we expand dim for anchors to [1, anchor_num, 4]\n",
    "        anchors_exp = np.expand_dims(anchors, axis=0)\n",
    "        sess,face_node_dict = model_restore_from_pb(face_pb_path, node_dict,GPU_ratio=GPU_ratio)\n",
    "        tf_input = face_node_dict['input']\n",
    "        shape = tf_input.shape\n",
    "        model_shape = [None,shape[1].value,shape[2].value,shape[3].value]\n",
    "#         print(\"model_shape = \", model_shape)\n",
    "        detection_bboxes = node_dict['detection_bboxes']\n",
    "        detection_scores = node_dict['detection_scores']\n",
    "        \n",
    "        self.margin = margin\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.iou_thresh = iou_thresh\n",
    "        self.anchors_exp = anchors_exp\n",
    "        self.model_shape = model_shape\n",
    "        self.tf_input = tf_input\n",
    "        self.sess = sess\n",
    "        self.detection_bboxes = detection_bboxes\n",
    "        self.detection_scores = detection_scores\n",
    "    def infer(self,img):\n",
    "        coors = list()\n",
    "        height,width,_ = img.shape\n",
    "        #----image processing\n",
    "        img_resized = cv2.resize(img, (self.model_shape[2], self.model_shape[1]))\n",
    "        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "        img_resized = img_resized.astype('float32')\n",
    "        img_resized /= 255\n",
    "        \n",
    "        #----mask detection\n",
    "        feed_dict={self.tf_input: np.expand_dims(img_resized, axis=0)}\n",
    "        y_bboxes_output, y_cls_output = self.sess.run([self.detection_bboxes, self.detection_scores],\n",
    "                                                 feed_dict=feed_dict)\n",
    "        y_bboxes = decode_bbox(self.anchors_exp, y_bboxes_output)[0]\n",
    "        y_cls = y_cls_output[0]\n",
    "        # To speed up, do single class NMS, not multiple classes NMS.\n",
    "        bbox_max_scores = np.max(y_cls, axis=1)\n",
    "        bbox_max_score_classes = np.argmax(y_cls, axis=1)\n",
    "\n",
    "        # keep_idx is the alive bounding box after nms.\n",
    "        keep_idxs = single_class_non_max_suppression(y_bboxes,\n",
    "                                                     bbox_max_scores,\n",
    "                                                     conf_thresh=self.conf_thresh,\n",
    "                                                     iou_thresh=self.iou_thresh,\n",
    "                                                     )\n",
    "        #====draw bounding box\n",
    "        for idx in keep_idxs:\n",
    "            conf = float(bbox_max_scores[idx])\n",
    "            class_id = bbox_max_score_classes[idx]\n",
    "            bbox = y_bboxes[idx]\n",
    "            # clip the coordinate, avoid the value exceed the image boundary.\n",
    "            xmin = max(0, int(bbox[0] * width - self.margin // 2))\n",
    "            ymin = max(0, int(bbox[1] * height - self.margin // 2))\n",
    "            xmax = min(int(bbox[2] * width + self.margin // 2), width)\n",
    "            ymax = min(int(bbox[3] * height + self.margin // 2), height)\n",
    "            coors.append((xmin,ymin,xmax,ymax))\n",
    "            \n",
    "        return coors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvsH-asoeLDZ"
   },
   "source": [
    "# 影像串流與口罩判斷的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XFE8F2uSeLDZ"
   },
   "outputs": [],
   "source": [
    "def mask_or_not(mask_pb_path,video_source=0,margin=40):\n",
    "    #----var\n",
    "    face_pb_path = \"face_detection.pb\"\n",
    "    frame_count = 0\n",
    "    FPS = \"Initialing\"\n",
    "    GPU_ratio = 0.06\n",
    "    nodename_dict = {\n",
    "                    'input': 'input:0',\n",
    "                    'keep_prob': 'keep_prob:0',\n",
    "                    'prediction': 'prediction:0'\n",
    "                     }\n",
    "    label2classname_dict = {0:'no_mask',\n",
    "                            1:\"with_mask\"}\n",
    "                        \n",
    "    #----影像串流初始化\n",
    "    cap, height, width = video_init(video_source)\n",
    "    print(\"影像高度:\",height)\n",
    "    print(\"影像寬度:\",width)\n",
    "\n",
    "    #----人臉偵測器初始化\n",
    "    find_face = face_detection(face_pb_path,margin=margin,\n",
    "                               GPU_ratio=GPU_ratio)\n",
    "\n",
    "    #----Mask PB檔案初始化\n",
    "    sess_infer,tf_node_dict = model_restore_from_pb(mask_pb_path,\n",
    "                            nodename_dict,GPU_ratio=GPU_ratio)\n",
    "    \n",
    "    #----取出推論的節點\n",
    "    pb_prediction = tf_node_dict['prediction']\n",
    "    pb_input = tf_node_dict['input']\n",
    "    pb_keep_prob = tf_node_dict['keep_prob']\n",
    "\n",
    "    #----建立不斷獲取影像的while迴圈\n",
    "    while (cap.isOpened()):\n",
    "\n",
    "        #----向cap獲取影像\n",
    "        ret, img = cap.read()\n",
    "        if ret is True:\n",
    "            #----人臉偵測\n",
    "            coors = find_face.infer(img)\n",
    "            \n",
    "            if len(coors):\n",
    "                for coor in coors:\n",
    "                    xmin,ymin,xmax,ymax = coor#臉部區域座標\n",
    "                    \n",
    "                    #----擷取臉部區域\n",
    "                    img_face = img[ymin:ymax,xmin:xmax,:].copy()\n",
    "                    #----調整大小至80 x 80\n",
    "                    img_face = cv2.resize(img_face,(80,80))\n",
    "                    #----將三維資料轉換成四維資料\n",
    "                    img_face = np.expand_dims(img_face,axis=0)\n",
    "                    #----將數值型態從uint8轉換成float32\n",
    "                    img_face = img_face.astype(np.float32)\n",
    "                    #----資料標準化(Normalization)\n",
    "                    img_face /= 255\n",
    "\n",
    "                    #----口罩偵測\n",
    "                    predictions = sess_infer.run(pb_prediction,\n",
    "                                 feed_dict={pb_input:img_face, pb_keep_prob:1})\n",
    "                    \n",
    "                    #----根據label轉換成類別名稱\n",
    "                    arg_predictions = np.argmax(predictions,axis=1)\n",
    "                    classname = label2classname_dict[arg_predictions[0]]\n",
    "                    #----根據類別名稱決定方框的顏色\n",
    "                    if classname == 'with_mask':#有戴口罩，方框為綠色\n",
    "                        color = (0, 255, 0)  # (B,G,R)\n",
    "                    else:#沒戴口罩，方框為紅色\n",
    "                        color = (0, 0, 255)  # (B,G,R)\n",
    "                    #----畫上方框\n",
    "                    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                    #----標註上類別名稱\n",
    "                    # cv2.putText(影像, 文字, 座標, 字型, 大小, 顏色, 線條寬度, 線條種類)\n",
    "                    cv2.putText(img, classname, (xmin + 2, ymin - 2),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, color)\n",
    "\n",
    "            #----FPS的計算\n",
    "            if frame_count == 0:\n",
    "                t_start = time.time()\n",
    "            frame_count += 1\n",
    "            if frame_count >= 20:\n",
    "                t_stop = time.time()\n",
    "                FPS = \"FPS={}\".format(round(20 / (t_stop - t_start)))\n",
    "                frame_count = 0\n",
    "\n",
    "            # cv2.putText(影像, 文字, 座標, 字型, 大小, 顏色, 線條寬度, 線條種類)\n",
    "            cv2.putText(img, FPS, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "\n",
    "            #----顯示影像\n",
    "            cv2.imshow(\"demo by JohnnyAI\", img)\n",
    "\n",
    "            #----按鍵偵測\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            print(\"取圖失敗\")\n",
    "            break\n",
    "\n",
    "    #----影像串流停止，釋放資源\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZpHSNeheLDa"
   },
   "source": [
    "# 函數的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SPbVx1hQeLDa",
    "outputId": "e04d301b-1ad1-433d-f516-6ff71669ee3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影像高度: 480\n",
      "影像寬度: 640\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-6a0435b3e435>:13: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_source = 0 #也可以貼上影片路徑(.mp4 或.avi)\n",
    "mask_pb_path = r\"infer_acc_0.99.pb\"\n",
    "margin = 40\n",
    "mask_or_not(mask_pb_path,video_source=video_source,margin=margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CH27_影像串流與口罩判斷.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
